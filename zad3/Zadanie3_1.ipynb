{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zadanie1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHru1_O8Fz-Z",
        "colab_type": "text"
      },
      "source": [
        "Michał Zych, michal.zych96@gmail.com\n",
        "\n",
        "Michał Warzecha, warzmich@gmail.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-I-UgtPFusJ",
        "colab_type": "code",
        "outputId": "341d240d-e9cb-405e-e078-cd49c289f0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQUwfO0DF4Rn",
        "colab_type": "code",
        "outputId": "6aa264fc-a6af-4899-8aaf-d751038ba915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qymHtGdRGEIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Z8nBEDGxg_",
        "colab_type": "code",
        "outputId": "a1774d2a-5cef-47dd-b5d3-a21640a1733b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def to_one_hot1(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    results = []\n",
        "    \n",
        "    for label in labels:\n",
        "        results.append(np.asarray([label==i for i in range(0,dimension)],dtype=np.uint8))\n",
        "        \n",
        "    return np.asarray(results)\n",
        "    \n",
        "# Our vectorized training labels\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "# Our vectorized test labels\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "\n",
        "print(one_hot_train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8982, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_cqMPXAHGd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89bgGpS3J5aZ",
        "colab_type": "text"
      },
      "source": [
        "Przygotowanie zbioru walidacyjnego"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfC-fSnYJ3ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SadIKHZIHV4v",
        "colab_type": "text"
      },
      "source": [
        "Po przygotowaniu danych można rozpacząć trening. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-pth1CjHiPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def create_model(nr_elements, nr_hidden_layers, activation):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(nr_elements, activation=activation, input_shape=(10000,)))\n",
        "  for i in range(nr_hidden_layers-1):\n",
        "    model.add(layers.Dense(nr_elements, activation=activation))\n",
        "  model.add(layers.Dense(46, activation='softmax'))\n",
        "  return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihi5PUfiMgRT",
        "colab_type": "code",
        "outputId": "c72606ed-db03-47b2-9c3d-870960da4906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# TEST FOR DIFFEREN NR ELEMENTS AND NR OF HIDDEN LAYER\n",
        "\n",
        "print(\"ALL NETWORKS ON THIS SET HAVE LOSS FUNCTION: CATEGORICAL_CROSSENTROPY AND ACTIVATION: RELU\")\n",
        "print(\"****************************************\\n\\n\")\n",
        "for nr_elements in [32, 64, 128]:\n",
        "  for nr_hidden_layers in [1,2,3]:\n",
        "    model = create_model(nr_elements, nr_hidden_layers, 'relu')\n",
        "    model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    print(\"Nr elements: \" + str(nr_elements) + \", nr hidden_layers: \" + str(nr_hidden_layers))\n",
        "    model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=0)          # TOO MANY ITERATIONS AND PRINT TRAINING WOULD BE UNREADABLE\n",
        "    predicts = model.predict(x_test)\n",
        "    predicted_labels = np.argmax(predicts, axis=1)\n",
        "    print(classification_report(test_labels, predicted_labels))\n",
        "    print(confusion_matrix(test_labels, predicted_labels))\n",
        "    print(\"****************************************\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ALL NETWORKS ON THIS SET HAVE LOSS FUNCTION: CATEGORICAL_CROSSENTROPY AND ACTIVATION: RELU\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 32, nr hidden_layers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.50      0.63        12\n",
            "           1       0.65      0.81      0.72       105\n",
            "           2       0.87      0.65      0.74        20\n",
            "           3       0.90      0.94      0.92       813\n",
            "           4       0.78      0.90      0.83       474\n",
            "           5       1.00      0.20      0.33         5\n",
            "           6       0.92      0.79      0.85        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.74      0.66      0.69        38\n",
            "           9       0.86      0.72      0.78        25\n",
            "          10       0.92      0.77      0.84        30\n",
            "          11       0.61      0.75      0.67        83\n",
            "          12       0.40      0.15      0.22        13\n",
            "          13       0.61      0.62      0.61        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.71      0.76      0.74        99\n",
            "          17       1.00      0.17      0.29        12\n",
            "          18       0.57      0.60      0.59        20\n",
            "          19       0.65      0.65      0.65       133\n",
            "          20       0.59      0.50      0.54        70\n",
            "          21       0.68      0.70      0.69        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.62      0.42      0.50        12\n",
            "          24       0.55      0.32      0.40        19\n",
            "          25       0.90      0.58      0.71        31\n",
            "          26       1.00      0.62      0.77         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.25      0.10      0.14        10\n",
            "          29       0.25      0.25      0.25         4\n",
            "          30       0.75      0.25      0.38        12\n",
            "          31       0.86      0.46      0.60        13\n",
            "          32       1.00      0.80      0.89        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.83      0.71      0.77         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.60      0.27      0.37        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       1.00      0.20      0.33        10\n",
            "          41       0.50      0.12      0.20         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.83      0.83      0.83         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.79      2246\n",
            "   macro avg       0.67      0.45      0.50      2246\n",
            "weighted avg       0.78      0.79      0.77      2246\n",
            "\n",
            "[[ 6  3  0 ...  0  0  0]\n",
            " [ 0 85  0 ...  0  0  0]\n",
            " [ 0  2 13 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  5  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 32, nr hidden_layers: 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.67      0.76        12\n",
            "           1       0.64      0.82      0.72       105\n",
            "           2       0.65      0.65      0.65        20\n",
            "           3       0.91      0.93      0.92       813\n",
            "           4       0.81      0.88      0.84       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.79      0.79      0.79        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.71      0.63      0.67        38\n",
            "           9       0.86      0.72      0.78        25\n",
            "          10       0.71      0.83      0.77        30\n",
            "          11       0.60      0.73      0.66        83\n",
            "          12       0.50      0.23      0.32        13\n",
            "          13       0.63      0.59      0.61        37\n",
            "          14       0.50      0.50      0.50         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.65      0.78      0.71        99\n",
            "          17       1.00      0.17      0.29        12\n",
            "          18       0.65      0.65      0.65        20\n",
            "          19       0.67      0.73      0.70       133\n",
            "          20       0.65      0.51      0.58        70\n",
            "          21       0.71      0.81      0.76        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.50      0.33      0.40        12\n",
            "          24       0.62      0.26      0.37        19\n",
            "          25       0.78      0.58      0.67        31\n",
            "          26       0.86      0.75      0.80         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.25      0.10      0.14        10\n",
            "          29       0.25      0.25      0.25         4\n",
            "          30       0.83      0.42      0.56        12\n",
            "          31       0.82      0.69      0.75        13\n",
            "          32       1.00      0.60      0.75        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.83      0.71      0.77         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.80      0.40      0.53        10\n",
            "          41       0.33      0.12      0.18         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.83      0.83      0.83         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.79      2246\n",
            "   macro avg       0.65      0.47      0.52      2246\n",
            "weighted avg       0.78      0.79      0.78      2246\n",
            "\n",
            "[[ 8  1  0 ...  0  0  0]\n",
            " [ 0 86  0 ...  0  0  0]\n",
            " [ 0  4 13 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  5  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 32, nr hidden_layers: 3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.42      0.56        12\n",
            "           1       0.70      0.74      0.72       105\n",
            "           2       0.71      0.50      0.59        20\n",
            "           3       0.85      0.94      0.89       813\n",
            "           4       0.81      0.84      0.83       474\n",
            "           5       1.00      0.20      0.33         5\n",
            "           6       0.85      0.79      0.81        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.72      0.61      0.66        38\n",
            "           9       0.78      0.56      0.65        25\n",
            "          10       0.95      0.60      0.73        30\n",
            "          11       0.52      0.75      0.61        83\n",
            "          12       0.11      0.08      0.09        13\n",
            "          13       0.46      0.49      0.47        37\n",
            "          14       1.00      0.50      0.67         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.53      0.74      0.62        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.69      0.55      0.61        20\n",
            "          19       0.69      0.62      0.65       133\n",
            "          20       0.66      0.39      0.49        70\n",
            "          21       0.77      0.63      0.69        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.33      0.17      0.22        12\n",
            "          24       0.50      0.42      0.46        19\n",
            "          25       0.76      0.52      0.62        31\n",
            "          26       1.00      0.25      0.40         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.50      0.10      0.17        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.56      0.42      0.48        12\n",
            "          31       0.86      0.46      0.60        13\n",
            "          32       0.83      0.50      0.62        10\n",
            "          33       1.00      0.60      0.75         5\n",
            "          34       0.62      0.71      0.67         7\n",
            "          35       1.00      0.33      0.50         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.44      0.40      0.42        10\n",
            "          41       0.67      0.25      0.36         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.75      1.00      0.86         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.75      2246\n",
            "   macro avg       0.63      0.42      0.47      2246\n",
            "weighted avg       0.75      0.75      0.74      2246\n",
            "\n",
            "[[ 5  2  0 ...  0  0  0]\n",
            " [ 0 78  0 ...  0  0  0]\n",
            " [ 0  1 10 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 64, nr hidden_layers: 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.67      0.76        12\n",
            "           1       0.66      0.82      0.73       105\n",
            "           2       0.81      0.65      0.72        20\n",
            "           3       0.92      0.93      0.92       813\n",
            "           4       0.83      0.87      0.85       474\n",
            "           5       1.00      0.20      0.33         5\n",
            "           6       0.93      0.93      0.93        14\n",
            "           7       0.50      0.33      0.40         3\n",
            "           8       0.75      0.63      0.69        38\n",
            "           9       0.88      0.84      0.86        25\n",
            "          10       0.90      0.90      0.90        30\n",
            "          11       0.57      0.80      0.67        83\n",
            "          12       0.50      0.15      0.24        13\n",
            "          13       0.59      0.62      0.61        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.66      0.76      0.70        99\n",
            "          17       0.75      0.25      0.38        12\n",
            "          18       0.65      0.55      0.59        20\n",
            "          19       0.60      0.74      0.66       133\n",
            "          20       0.65      0.44      0.53        70\n",
            "          21       0.66      0.70      0.68        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.58      0.58      0.58        12\n",
            "          24       0.55      0.32      0.40        19\n",
            "          25       0.81      0.68      0.74        31\n",
            "          26       1.00      0.88      0.93         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.25      0.10      0.14        10\n",
            "          29       0.20      0.25      0.22         4\n",
            "          30       0.71      0.42      0.53        12\n",
            "          31       0.88      0.54      0.67        13\n",
            "          32       1.00      0.80      0.89        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.83      0.71      0.77         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.60      0.27      0.37        11\n",
            "          37       0.33      0.50      0.40         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       1.00      0.20      0.33         5\n",
            "          40       1.00      0.20      0.33        10\n",
            "          41       0.67      0.25      0.36         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.75      1.00      0.86         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80      2246\n",
            "   macro avg       0.70      0.51      0.55      2246\n",
            "weighted avg       0.80      0.80      0.79      2246\n",
            "\n",
            "[[ 8  1  0 ...  0  0  0]\n",
            " [ 0 86  0 ...  0  0  0]\n",
            " [ 0  2 13 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 64, nr hidden_layers: 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70        12\n",
            "           1       0.69      0.78      0.74       105\n",
            "           2       0.56      0.70      0.62        20\n",
            "           3       0.93      0.89      0.91       813\n",
            "           4       0.70      0.91      0.79       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.93      0.93      0.93        14\n",
            "           7       0.50      0.33      0.40         3\n",
            "           8       0.69      0.71      0.70        38\n",
            "           9       0.86      0.72      0.78        25\n",
            "          10       0.96      0.73      0.83        30\n",
            "          11       0.55      0.80      0.65        83\n",
            "          12       0.50      0.23      0.32        13\n",
            "          13       0.61      0.62      0.61        37\n",
            "          14       0.20      0.50      0.29         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.70      0.72      0.71        99\n",
            "          17       1.00      0.25      0.40        12\n",
            "          18       0.60      0.60      0.60        20\n",
            "          19       0.73      0.56      0.64       133\n",
            "          20       0.67      0.47      0.55        70\n",
            "          21       0.69      0.74      0.71        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.56      0.42      0.48        12\n",
            "          24       0.50      0.37      0.42        19\n",
            "          25       0.78      0.68      0.72        31\n",
            "          26       1.00      0.88      0.93         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.29      0.20      0.24        10\n",
            "          29       0.40      0.50      0.44         4\n",
            "          30       1.00      0.25      0.40        12\n",
            "          31       0.86      0.46      0.60        13\n",
            "          32       1.00      0.60      0.75        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.80      0.57      0.67         7\n",
            "          35       1.00      0.33      0.50         6\n",
            "          36       0.67      0.18      0.29        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.57      0.40      0.47        10\n",
            "          41       0.50      0.25      0.33         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.75      1.00      0.86         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.78      2246\n",
            "   macro avg       0.65      0.49      0.53      2246\n",
            "weighted avg       0.78      0.78      0.77      2246\n",
            "\n",
            "[[ 7  1  1 ...  0  0  0]\n",
            " [ 0 82  1 ...  0  0  0]\n",
            " [ 0  2 14 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 64, nr hidden_layers: 3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70        12\n",
            "           1       0.69      0.78      0.73       105\n",
            "           2       0.86      0.60      0.71        20\n",
            "           3       0.88      0.93      0.90       813\n",
            "           4       0.79      0.87      0.83       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.92      0.79      0.85        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.74      0.68      0.71        38\n",
            "           9       0.83      0.76      0.79        25\n",
            "          10       0.62      0.93      0.75        30\n",
            "          11       0.76      0.54      0.63        83\n",
            "          12       0.20      0.08      0.11        13\n",
            "          13       0.58      0.59      0.59        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.69      0.70      0.69        99\n",
            "          17       0.57      0.33      0.42        12\n",
            "          18       0.55      0.60      0.57        20\n",
            "          19       0.56      0.74      0.64       133\n",
            "          20       0.59      0.33      0.42        70\n",
            "          21       0.59      0.59      0.59        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.33      0.25      0.29        12\n",
            "          24       0.57      0.42      0.48        19\n",
            "          25       0.72      0.58      0.64        31\n",
            "          26       1.00      0.75      0.86         8\n",
            "          27       0.33      0.25      0.29         4\n",
            "          28       0.60      0.30      0.40        10\n",
            "          29       0.40      0.50      0.44         4\n",
            "          30       0.80      0.33      0.47        12\n",
            "          31       0.71      0.38      0.50        13\n",
            "          32       0.89      0.80      0.84        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.50      0.14      0.22         7\n",
            "          35       0.67      0.33      0.44         6\n",
            "          36       0.40      0.55      0.46        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.67      0.80         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.60      0.30      0.40        10\n",
            "          41       0.40      0.25      0.31         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.83      0.83      0.83         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.77      2246\n",
            "   macro avg       0.60      0.47      0.51      2246\n",
            "weighted avg       0.76      0.77      0.76      2246\n",
            "\n",
            "[[ 7  1  0 ...  0  0  0]\n",
            " [ 0 82  0 ...  0  0  0]\n",
            " [ 0  3 12 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  5  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 128, nr hidden_layers: 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.67      0.70        12\n",
            "           1       0.64      0.83      0.73       105\n",
            "           2       0.80      0.60      0.69        20\n",
            "           3       0.91      0.93      0.92       813\n",
            "           4       0.81      0.88      0.85       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.93      0.93      0.93        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.70      0.68      0.69        38\n",
            "           9       0.84      0.84      0.84        25\n",
            "          10       0.93      0.87      0.90        30\n",
            "          11       0.57      0.75      0.65        83\n",
            "          12       0.50      0.15      0.24        13\n",
            "          13       0.63      0.65      0.64        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.69      0.75      0.71        99\n",
            "          17       0.75      0.25      0.38        12\n",
            "          18       0.76      0.65      0.70        20\n",
            "          19       0.67      0.67      0.67       133\n",
            "          20       0.52      0.53      0.52        70\n",
            "          21       0.72      0.67      0.69        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.60      0.50      0.55        12\n",
            "          24       0.50      0.26      0.34        19\n",
            "          25       0.79      0.61      0.69        31\n",
            "          26       1.00      0.88      0.93         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.40      0.20      0.27        10\n",
            "          29       0.25      0.25      0.25         4\n",
            "          30       0.55      0.50      0.52        12\n",
            "          31       0.88      0.54      0.67        13\n",
            "          32       1.00      0.70      0.82        10\n",
            "          33       0.80      0.80      0.80         5\n",
            "          34       0.75      0.43      0.55         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.57      0.36      0.44        11\n",
            "          37       0.33      0.50      0.40         2\n",
            "          38       1.00      0.67      0.80         3\n",
            "          39       1.00      0.20      0.33         5\n",
            "          40       1.00      0.30      0.46        10\n",
            "          41       0.50      0.25      0.33         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.86      1.00      0.92         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.79      2246\n",
            "   macro avg       0.68      0.50      0.55      2246\n",
            "weighted avg       0.79      0.79      0.78      2246\n",
            "\n",
            "[[ 8  2  0 ...  0  0  0]\n",
            " [ 0 87  0 ...  0  0  0]\n",
            " [ 0  2 12 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 128, nr hidden_layers: 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.75      0.82        12\n",
            "           1       0.69      0.78      0.74       105\n",
            "           2       0.80      0.60      0.69        20\n",
            "           3       0.90      0.94      0.92       813\n",
            "           4       0.81      0.88      0.85       474\n",
            "           5       1.00      0.20      0.33         5\n",
            "           6       0.86      0.86      0.86        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.55      0.74      0.63        38\n",
            "           9       0.87      0.80      0.83        25\n",
            "          10       0.88      0.97      0.92        30\n",
            "          11       0.63      0.75      0.68        83\n",
            "          12       0.33      0.15      0.21        13\n",
            "          13       0.60      0.70      0.65        37\n",
            "          14       0.25      0.50      0.33         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.73      0.74      0.73        99\n",
            "          17       0.67      0.17      0.27        12\n",
            "          18       0.52      0.60      0.56        20\n",
            "          19       0.66      0.72      0.69       133\n",
            "          20       0.69      0.36      0.47        70\n",
            "          21       0.73      0.59      0.65        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.50      0.25      0.33        12\n",
            "          24       0.62      0.42      0.50        19\n",
            "          25       0.83      0.61      0.70        31\n",
            "          26       1.00      0.75      0.86         8\n",
            "          27       1.00      0.50      0.67         4\n",
            "          28       0.43      0.30      0.35        10\n",
            "          29       0.50      0.75      0.60         4\n",
            "          30       0.83      0.42      0.56        12\n",
            "          31       0.78      0.54      0.64        13\n",
            "          32       1.00      0.80      0.89        10\n",
            "          33       0.80      0.80      0.80         5\n",
            "          34       0.67      0.57      0.62         7\n",
            "          35       1.00      0.50      0.67         6\n",
            "          36       0.57      0.36      0.44        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       1.00      0.40      0.57         5\n",
            "          40       1.00      0.40      0.57        10\n",
            "          41       0.50      0.25      0.33         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.86      1.00      0.92         6\n",
            "          44       0.67      0.80      0.73         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80      2246\n",
            "   macro avg       0.70      0.53      0.57      2246\n",
            "weighted avg       0.79      0.80      0.79      2246\n",
            "\n",
            "[[ 9  1  0 ...  0  0  0]\n",
            " [ 0 82  0 ...  0  0  0]\n",
            " [ 0  1 12 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "Nr elements: 128, nr hidden_layers: 3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70        12\n",
            "           1       0.59      0.83      0.69       105\n",
            "           2       0.62      0.65      0.63        20\n",
            "           3       0.92      0.92      0.92       813\n",
            "           4       0.80      0.88      0.84       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.92      0.86      0.89        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.66      0.66      0.66        38\n",
            "           9       0.69      0.88      0.77        25\n",
            "          10       0.89      0.83      0.86        30\n",
            "          11       0.67      0.65      0.66        83\n",
            "          12       0.50      0.15      0.24        13\n",
            "          13       0.52      0.65      0.58        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.60      0.78      0.68        99\n",
            "          17       1.00      0.33      0.50        12\n",
            "          18       0.69      0.55      0.61        20\n",
            "          19       0.63      0.73      0.68       133\n",
            "          20       0.72      0.49      0.58        70\n",
            "          21       0.83      0.56      0.67        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.50      0.33      0.40        12\n",
            "          24       0.57      0.21      0.31        19\n",
            "          25       0.82      0.74      0.78        31\n",
            "          26       0.75      0.75      0.75         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.33      0.10      0.15        10\n",
            "          29       0.50      0.75      0.60         4\n",
            "          30       1.00      0.33      0.50        12\n",
            "          31       0.88      0.54      0.67        13\n",
            "          32       1.00      0.80      0.89        10\n",
            "          33       0.80      0.80      0.80         5\n",
            "          34       0.67      0.29      0.40         7\n",
            "          35       1.00      0.33      0.50         6\n",
            "          36       0.43      0.27      0.33        11\n",
            "          37       0.33      0.50      0.40         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       0.50      0.20      0.29         5\n",
            "          40       1.00      0.20      0.33        10\n",
            "          41       0.67      0.25      0.36         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.75      1.00      0.86         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.79      2246\n",
            "   macro avg       0.68      0.49      0.53      2246\n",
            "weighted avg       0.79      0.79      0.78      2246\n",
            "\n",
            "[[ 7  1  0 ...  0  0  0]\n",
            " [ 0 87  0 ...  0  0  0]\n",
            " [ 0  2 13 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  1  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFAifUogRP2x",
        "colab_type": "text"
      },
      "source": [
        "Wszystkie sieci wykazały zbliżony poziom dokładnosci (między 80 a 75 % poprawnej predykcji) co ciekawe najlepszą skuteczność miały sieci jednowarstwowe ewentualnie dwuwarstwowe. Możliwe że dołożenie trzeciej warstwy przy takiej liczbie danych przyczyniło się do delikatnego przetrenowania modelu i gorszej dokładności"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkfM-9YmRmIy",
        "colab_type": "code",
        "outputId": "e68f1277-eb6a-4953-ab5d-f0be4b21754e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#TEST FOR OTHER ACTIVATION (TANH) AND OTHER LOSS FUNCTION (MSE)\n",
        "\n",
        "\n",
        "print(\"ACTIVATION: TANH\")\n",
        "model = create_model(64, 2, 'tanh')\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(\"Nr elements: \" + str(64) + \", nr hidden_layers: \" + str(2))\n",
        "model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=0)          # TOO MANY ITERATIONS AND PRINT TRAINING WOULD BE UNREADABLE\n",
        "predicts = model.predict(x_test)\n",
        "predicted_labels = np.argmax(predicts, axis=1)\n",
        "print(classification_report(test_labels, predicted_labels))\n",
        "print(confusion_matrix(test_labels, predicted_labels))\n",
        "\n",
        "\n",
        "print(\"****************************************\\n\\n\")\n",
        "print(\"LOSS FUNCTION: MSE\")\n",
        "model = create_model(64, 2, 'relu')\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "print(\"Nr elements: \" + str(64) + \", nr hidden_layers: \" + str(2))\n",
        "model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=0)          # TOO MANY ITERATIONS AND PRINT TRAINING WOULD BE UNREADABLE\n",
        "predicts = model.predict(x_test)\n",
        "predicted_labels = np.argmax(predicts, axis=1)\n",
        "print(classification_report(test_labels, predicted_labels))\n",
        "print(confusion_matrix(test_labels, predicted_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACTIVATION: TANH\n",
            "Nr elements: 64, nr hidden_layers: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.50      0.63        12\n",
            "           1       0.66      0.81      0.73       105\n",
            "           2       0.73      0.55      0.63        20\n",
            "           3       0.92      0.91      0.91       813\n",
            "           4       0.79      0.89      0.84       474\n",
            "           5       0.50      0.20      0.29         5\n",
            "           6       0.92      0.86      0.89        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.70      0.68      0.69        38\n",
            "           9       0.80      0.80      0.80        25\n",
            "          10       0.86      0.83      0.85        30\n",
            "          11       0.61      0.72      0.66        83\n",
            "          12       0.40      0.15      0.22        13\n",
            "          13       0.55      0.57      0.56        37\n",
            "          14       1.00      0.50      0.67         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.64      0.76      0.69        99\n",
            "          17       0.60      0.25      0.35        12\n",
            "          18       0.63      0.60      0.62        20\n",
            "          19       0.62      0.72      0.67       133\n",
            "          20       0.64      0.46      0.53        70\n",
            "          21       0.76      0.70      0.73        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.62      0.42      0.50        12\n",
            "          24       0.70      0.37      0.48        19\n",
            "          25       0.84      0.68      0.75        31\n",
            "          26       0.71      0.62      0.67         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.40      0.20      0.27        10\n",
            "          29       0.40      0.50      0.44         4\n",
            "          30       0.71      0.42      0.53        12\n",
            "          31       0.64      0.54      0.58        13\n",
            "          32       1.00      0.60      0.75        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.75      0.43      0.55         7\n",
            "          35       1.00      0.33      0.50         6\n",
            "          36       0.67      0.36      0.47        11\n",
            "          37       0.20      0.50      0.29         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       1.00      0.20      0.33        10\n",
            "          41       0.50      0.12      0.20         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.86      1.00      0.92         6\n",
            "          44       0.67      0.80      0.73         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.78      2246\n",
            "   macro avg       0.68      0.49      0.54      2246\n",
            "weighted avg       0.78      0.78      0.78      2246\n",
            "\n",
            "[[ 6  1  0 ...  0  0  0]\n",
            " [ 0 85  0 ...  0  0  0]\n",
            " [ 0  4 11 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "****************************************\n",
            "\n",
            "\n",
            "LOSS FUNCTION: MSE\n",
            "Nr elements: 64, nr hidden_layers: 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.67      0.76        12\n",
            "           1       0.63      0.79      0.70       105\n",
            "           2       0.04      0.05      0.04        20\n",
            "           3       0.93      0.94      0.93       813\n",
            "           4       0.83      0.89      0.86       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.65      0.93      0.76        14\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.66      0.66      0.66        38\n",
            "           9       0.81      0.84      0.82        25\n",
            "          10       0.61      0.83      0.70        30\n",
            "          11       0.60      0.70      0.64        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.51      0.62      0.56        37\n",
            "          14       0.50      0.50      0.50         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.66      0.79      0.72        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.75      0.60      0.67        20\n",
            "          19       0.65      0.66      0.65       133\n",
            "          20       0.48      0.51      0.50        70\n",
            "          21       0.55      0.78      0.65        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.00      0.00      0.00        12\n",
            "          24       0.42      0.26      0.32        19\n",
            "          25       0.81      0.71      0.76        31\n",
            "          26       0.00      0.00      0.00         8\n",
            "          27       0.00      0.00      0.00         4\n",
            "          28       0.04      0.10      0.05        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.40      0.67      0.50        12\n",
            "          31       0.00      0.00      0.00        13\n",
            "          32       0.00      0.00      0.00        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.67      0.57      0.62         7\n",
            "          35       0.00      0.00      0.00         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.67      0.80         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.00      0.00      0.00         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.77      2246\n",
            "   macro avg       0.33      0.34      0.33      2246\n",
            "weighted avg       0.73      0.77      0.75      2246\n",
            "\n",
            "[[ 8  2  0 ...  0  0  0]\n",
            " [ 0 83  0 ...  0  0  0]\n",
            " [ 0  3  1 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  1 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m83ZVcs0Shwz",
        "colab_type": "text"
      },
      "source": [
        "Dla pokazanego powyżej przykładu (liczba warstw = 2 liczba elementów w warstwie = 64) zmiana funkcji aktywacji z relu na tanh nie zmieniła dokładności modelu (dalej wynosi ona 78%).\n",
        "\n",
        "Zmiana loss function z crossentropy_categorical na MSE spowodowała drobne pogorszenie dokładności modelu (spadek z 78 do 77%)"
      ]
    }
  ]
}